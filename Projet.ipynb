{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from impyute.imputation.cs import mice#\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer#\n",
    "import missingno as msno#\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from missingpy import KNNImputer\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.set_option(\"display.max_rows\", 8)\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "from missingpy import MissForest\n",
    "import io\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn import decomposition\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dapp = pd.read_csv(\"horse-colic.data\" , sep = \"\\s+\", names = [\"V\"+str(i) for i in range(28)])\n",
    "Dtest = pd.read_csv(\"horse-colic.test\" , sep = \"\\s+\", names = [\"V\"+str(i) for i in range(28)])\n",
    "liste_drop = [\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\"]\n",
    "Dapp = Dapp.drop(liste_drop , axis = 1)\n",
    "Dtest = Dtest.drop(liste_drop , axis = 1)\n",
    "Dapp = Dapp.replace(\"?\",np.nan)\n",
    "Dtest = Dtest.replace(\"?\",np.nan)\n",
    "\n",
    "#Dtest\n",
    "#Dapp\n",
    "#On utilisera une méthode d'imputation qui gère différentes types pour les variables ! (cf Référence)\n",
    "#On se doit tout de même spécifier le type des variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "liste_categorical = [] ; liste_numerical = []\n",
    "for i in range(22):\n",
    "    if(i in list(range(3)) + list(range(6 , 15)) + [16 , 17 , 20]):\n",
    "        d[\"V\"+str(i)] = \"category\"\n",
    "        liste_categorical.append(\"V\"+str(i))\n",
    "    else:\n",
    "        d[\"V\"+str(i)] = \"float64\"\n",
    "        liste_numerical.append(\"V\"+str(i))\n",
    "liste_categorical\n",
    "liste_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapp = Dapp.astype(d)\n",
    "Dtest = Dtest.astype(d)\n",
    "Dapp_num = Dapp.select_dtypes(include=['float64']) ; Dapp_cat = Dapp.select_dtypes(include=['category'])\n",
    "Dtest_num = Dtest.select_dtypes(include=['float64']) ; Dtest_cat = Dtest.select_dtypes(include=['category'])\n",
    "Dtest_cat.loc[: , \"V9\"] = pd.Series(pd.Categorical(Dtest_cat.loc[: , \"V9\"], categories=[\"1\",\"2\",\"3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(l):\n",
    "    return np.array([float(x) for x in l])\n",
    "\n",
    "imputed_training=mice(np.array(list(map(g , Dapp_num.values))))\n",
    "imputed_test = mice(np.array(list(map(g , Dtest_num.values))))\n",
    "\n",
    "Dapp_num_imputed = pd.DataFrame(list(map(np.ravel, (list(imputed_training))))).round(2)\n",
    "Dapp_num_imputed.columns = liste_numerical\n",
    "\n",
    "Dtest_num_imputed = pd.DataFrame(list(map(np.ravel, (list(imputed_test))))).round(2)\n",
    "Dtest_num_imputed.columns = liste_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On a généré quelques valeurs négatives, il est nécessaire d'effectuer de nouvelles imputations sur nos données\n",
    "Dapp_num_imputed[Dapp_num_imputed < 0] = np.nan\n",
    "Dtest_num_imputed[Dtest_num_imputed < 0] = np.nan\n",
    "\n",
    "distances_app = pdist(Dapp_num_imputed.values, metric='euclidean')\n",
    "dist_matrix_app = squareform(distances_app)\n",
    "matrice_distance_app = pd.DataFrame(list(map(np.ravel, (list(dist_matrix_app))))).round(2)\n",
    "\n",
    "distances_test = pdist(Dtest_num_imputed.values, metric='euclidean')\n",
    "dist_matrix_test = squareform(distances_test)\n",
    "matrice_distance_test = pd.DataFrame(list(map(np.ravel, (list(dist_matrix_test))))).round(2)\n",
    "#on ne peut pas se permettre de prendre des poids uniformes\n",
    "#en effet, il se peut que le 2e plus proche soit en réalité très loin du point par rapport au premier\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=15, weights=\"distance\")\n",
    "Dapp_num_imputed = pd.DataFrame(list(map(np.ravel, (list(imputer.fit_transform(Dapp_num_imputed)))))).round(2)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
    "Dtest_num_imputed = pd.DataFrame(list(map(np.ravel, (list(imputer.fit_transform(Dtest_num_imputed)))))).round(2)\n",
    "\n",
    "Dapp_num_imputed.columns = liste_numerical ; Dtest_num_imputed.columns = liste_numerical\n",
    "#le choix du nombre de voisins est difficile, on fait donc des choix arbitraires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{lab : 1-(len(Dtest_num[lab].dropna())/len(Dtest_num)) for lab in liste_numerical} \n",
    "{lab : 1-(len(Dapp_num[lab].dropna())/len(Dapp_num)) for lab in liste_numerical} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dapp_num.V21\n",
    "y = Dapp_num_imputed.V21\n",
    "#L'objectif est de comparer les distributions pour voir si l'on observe une certaine cohérence après imputation\n",
    "#on compare les distributions avant/après\n",
    "#on calcule aussi la proportion de données manquantes pour être sûr que la comparaison ait un sens.\n",
    "#il faut que le % de données manquantes ne soit ni trop faible ni trop élevé pour pouvoir comparer\n",
    "#on peut faire une \"évaluation\" visuelle de notre imputation sur cette variable choisie\n",
    "plt.hist([x, y], label=['Missing data', 'Imputed data'] , color = [\"plum\",\"peru\"])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##La variable ID hôpital sera supprimée, qui peut être considéré (globalement) comme un identifiant pour les chevaux\n",
    "##en effet, si l'on conserve cette variable en termes d'ajustement, on sera performant.\n",
    "##cependant, nos algorithmes de prédiction ne seront pas bons d'un point de vue de la généralisation\n",
    "#62 lignes des données de test sont des nouveaux identifiants -- > overfitting\n",
    "\n",
    "#Réf : Unleash Machine Learning Techniques, De Raghav Bali, Dipanjan Sarkar, Brett Lantz p.393\n",
    "#Advances in Computing and Information Technolog, Natarajan Meghanathan, Dhinaharan Nagamalai, Nabendu Chaki P.352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtest_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapp_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dapp_cat = Dapp_cat.drop([\"V2\"] , axis = 1) ; Dtest_cat = Dtest_cat.drop([\"V2\"] , axis = 1)\n",
    "cat_cols = [Dapp_cat.columns.get_loc(col) for col in Dapp_cat.select_dtypes(['category']).columns.tolist()]\n",
    "\n",
    "imputer = MissForest(random_state = 100)\n",
    "Dapp_cat_imputed = imputer.fit_transform(Dapp_cat , cat_vars = cat_cols)\n",
    "Dapp_cat_imputed = pd.DataFrame(list(map(np.ravel, (list(Dapp_cat_imputed)))))\n",
    "Dapp_cat_imputed.columns = [x for x in liste_categorical if(x!=\"V2\")] #if(x!=\"V2\")\n",
    "Dapp_cat_imputed = Dapp_cat_imputed.apply(lambda x : x.astype(int).astype(str).astype(\"category\") , axis = 1)\n",
    "\n",
    "imputer = MissForest(random_state = 100)\n",
    "Dtest_cat_imputed = imputer.fit_transform(Dtest_cat , cat_vars = cat_cols)\n",
    "Dtest_cat_imputed = pd.DataFrame(list(map(np.ravel, (list(Dtest_cat_imputed)))))\n",
    "Dtest_cat_imputed.columns = [x for x in liste_categorical if(x!=\"V2\")] #if(x!=\"V2\")\n",
    "Dtest_cat_imputed = Dtest_cat_imputed.apply(lambda x : x.astype(int).astype(str).astype(\"category\") , axis = 1)\n",
    "Dtest_cat_imputed.loc[: , \"V9\"] = pd.Series(pd.Categorical(Dtest_cat_imputed.loc[: , \"V9\"], categories=[\"1\",\"2\",\"3\"]))\n",
    "#une modalité de V9 n'est pas présente dans la variable V9 des données de test : il faut rajouter une catégorie\n",
    "#pour la variable V9 des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{lab : 1-(len(Dtest_cat[lab].dropna())/len(Dtest)) for lab in liste_categorical if(lab != \"V2\")} \n",
    "{lab : 1-(len(Dapp_cat[lab].dropna())/len(Dapp)) for lab in liste_categorical if(lab != \"V2\")}\n",
    "#les proportions de données manquantes pour chaque varible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapp_cat_imputed.V13.value_counts().sort_index().plot(kind = \"bar\")\n",
    "#en pratique, il est nécessaire de vérifier la distribution des variables avant\\après imputation\n",
    "#on trace donc des barplot pour avoir une idée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapp_cat[\"V13\"].value_counts().sort_index().plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapp_imputed = pd.concat([Dapp_num_imputed.reset_index(drop=True), Dapp_cat_imputed], axis=1)\n",
    "Dapp_imputed.columns = [int(x[1:]) for x in Dapp_imputed.columns]\n",
    "Dapp_imputed = Dapp_imputed.sort_index(axis = 1)\n",
    "Dapp_imputed.columns = [\"V\"+str(x) for x in Dapp_imputed.columns]\n",
    "\n",
    "Dtest_imputed = pd.concat([Dtest_num_imputed.reset_index(drop=True), Dtest_cat_imputed], axis=1)\n",
    "Dtest_imputed.columns = [int(x[1:]) for x in Dtest_imputed.columns]\n",
    "Dtest_imputed = Dtest_imputed.sort_index(axis = 1)\n",
    "Dtest_imputed.columns = [\"V\"+str(x) for x in Dtest_imputed.columns]\n",
    "Dtest_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapp_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dtest_imputed.to_pickle(\"Dtest_imputed.pkl\")\n",
    "Dapp_imputed.to_pickle(\"Dapp_imputed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dapp_imputed = pd.read_pickle(\"Dapp_imputed.pkl\")\n",
    "Dtest_imputed = pd.read_pickle(\"Dtest_imputed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_cat_bis = [x for x in liste_categorical if(x not in [\"V0\" , \"V2\"] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V15</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V21</th>\n",
       "      <th>V1_9</th>\n",
       "      <th>V6_2</th>\n",
       "      <th>...</th>\n",
       "      <th>V14_3</th>\n",
       "      <th>V16_2</th>\n",
       "      <th>V16_3</th>\n",
       "      <th>V16_4</th>\n",
       "      <th>V17_2</th>\n",
       "      <th>V17_3</th>\n",
       "      <th>V17_4</th>\n",
       "      <th>V17_5</th>\n",
       "      <th>V20_2</th>\n",
       "      <th>V20_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500536</td>\n",
       "      <td>-0.647548</td>\n",
       "      <td>-0.651004</td>\n",
       "      <td>0.678060</td>\n",
       "      <td>-0.431817</td>\n",
       "      <td>-0.695687</td>\n",
       "      <td>-0.777540</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.867880</td>\n",
       "      <td>-0.862987</td>\n",
       "      <td>0.338104</td>\n",
       "      <td>-0.323003</td>\n",
       "      <td>-0.230760</td>\n",
       "      <td>-0.695687</td>\n",
       "      <td>1.401894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.715834</td>\n",
       "      <td>-1.006613</td>\n",
       "      <td>-0.156450</td>\n",
       "      <td>-0.348937</td>\n",
       "      <td>-0.130231</td>\n",
       "      <td>1.761921</td>\n",
       "      <td>-0.687021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.780158</td>\n",
       "      <td>-0.575735</td>\n",
       "      <td>-0.403727</td>\n",
       "      <td>-0.037726</td>\n",
       "      <td>-1.135516</td>\n",
       "      <td>1.414692</td>\n",
       "      <td>-0.687021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.563788</td>\n",
       "      <td>-1.078426</td>\n",
       "      <td>0.585381</td>\n",
       "      <td>0.169749</td>\n",
       "      <td>-1.034987</td>\n",
       "      <td>-0.699545</td>\n",
       "      <td>-0.596501</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.259695</td>\n",
       "      <td>-0.432109</td>\n",
       "      <td>-1.145558</td>\n",
       "      <td>0.133441</td>\n",
       "      <td>-0.230760</td>\n",
       "      <td>1.569016</td>\n",
       "      <td>-0.687021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.259695</td>\n",
       "      <td>-1.078426</td>\n",
       "      <td>-1.145558</td>\n",
       "      <td>0.392783</td>\n",
       "      <td>-0.934459</td>\n",
       "      <td>-0.714978</td>\n",
       "      <td>-0.366721</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.867880</td>\n",
       "      <td>0.573272</td>\n",
       "      <td>0.338104</td>\n",
       "      <td>-1.604156</td>\n",
       "      <td>-0.230760</td>\n",
       "      <td>-0.707261</td>\n",
       "      <td>6.603291</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V0        V3        V4        V5       V15       V18       V19       V21  \\\n",
       "0    0  0.500536 -0.647548 -0.651004  0.678060 -0.431817 -0.695687 -0.777540   \n",
       "1    0 -0.867880 -0.862987  0.338104 -0.323003 -0.230760 -0.695687  1.401894   \n",
       "2    1 -0.715834 -1.006613 -0.156450 -0.348937 -0.130231  1.761921 -0.687021   \n",
       "3    1 -1.780158 -0.575735 -0.403727 -0.037726 -1.135516  1.414692 -0.687021   \n",
       "..  ..       ...       ...       ...       ...       ...       ...       ...   \n",
       "64   0 -0.563788 -1.078426  0.585381  0.169749 -1.034987 -0.699545 -0.596501   \n",
       "65   1 -0.259695 -0.432109 -1.145558  0.133441 -0.230760  1.569016 -0.687021   \n",
       "66   0 -0.259695 -1.078426 -1.145558  0.392783 -0.934459 -0.714978 -0.366721   \n",
       "67   0 -0.867880  0.573272  0.338104 -1.604156 -0.230760 -0.707261  6.603291   \n",
       "\n",
       "    V1_9  V6_2  ...  V14_3  V16_2  V16_3  V16_4  V17_2  V17_3  V17_4  V17_5  \\\n",
       "0      0     0  ...      0      0      0      1      1      0      0      0   \n",
       "1      0     0  ...      0      0      1      0      0      0      0      1   \n",
       "2      0     0  ...      0      0      1      0      0      0      0      1   \n",
       "3      0     0  ...      0      0      1      0      0      0      0      1   \n",
       "..   ...   ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "64     0     0  ...      0      0      1      0      0      1      0      0   \n",
       "65     0     0  ...      0      0      0      0      0      0      1      0   \n",
       "66     0     0  ...      0      0      0      0      0      0      0      0   \n",
       "67     0     0  ...      1      0      0      1      0      0      1      0   \n",
       "\n",
       "    V20_2  V20_3  \n",
       "0       0      1  \n",
       "1       0      0  \n",
       "2       0      1  \n",
       "3       0      1  \n",
       "..    ...    ...  \n",
       "64      0      0  \n",
       "65      0      1  \n",
       "66      0      0  \n",
       "67      1      0  \n",
       "\n",
       "[68 rows x 45 columns]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Régression logistique\n",
    "scaler = StandardScaler()\n",
    "Dapp_imputed[liste_numerical] = scaler.fit_transform(Dapp_imputed[liste_numerical])\n",
    "Dapp_imputed[\"V0\"] = pd.Series([1 if(x==\"1\") else 0 for x in Dapp_imputed[\"V0\"]])\n",
    "Dapp_imputed = Dapp_imputed.astype({\"V0\" : \"int32\"})\n",
    "Dapp_imputed = pd.get_dummies(Dapp_imputed , columns = liste_cat_bis , drop_first = True)\n",
    "\n",
    "Dtest_imputed[liste_numerical] = scaler.transform(Dtest_imputed[liste_numerical])\n",
    "Dtest_imputed[\"V0\"] = pd.Series([1 if(x==\"1\") else 0 for x in Dtest_imputed[\"V0\"]])\n",
    "Dtest_imputed = Dtest_imputed.astype({\"V0\" : \"int32\"})\n",
    "Dtest_imputed = pd.get_dummies(Dtest_imputed , columns = liste_cat_bis , drop_first = True)\n",
    "\n",
    "Dapp_imputed\n",
    "Dtest_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = Dapp_imputed.drop([\"V0\"] , axis = 1) ; Y_train = Dapp_imputed[\"V0\"]\n",
    "X_test = Dtest_imputed.drop([\"V0\"] , axis = 1) ; Y_test = Dtest_imputed[\"V0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2 approches seront utilisées pour choisir les paramètres : - LOOCV\n",
    "## en théorie, on pourrait considérer que nous avons un petit échantillon : on devrait effectuer des validations LOO\n",
    "#pour déterminer les paramètres de nos méthodes\n",
    "#on essaiera aussi des choix arbitraires de paramètres sur les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205882352941176"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def param_selection_log(X, y,Cs):\n",
    "    logreg = LogisticRegression(penalty = \"l2\" , solver = \"lbfgs\" , max_iter = 500 , tol = 1e-6)\n",
    "    parameters = {'C':Cs}\n",
    "    clf = GridSearchCV(logreg, parameters , return_train_score = True , cv = KFold(n_splits = 300))\n",
    "    clf.fit(X_train, Y_train)\n",
    "    return clf.best_params_\n",
    "C = param_selection_log(X_train, Y_train , [0.01 , 0.1 , 10 , 100])[\"C\"]\n",
    "\n",
    "logreg = LogisticRegression(penalty = \"l2\" , C = C,solver = \"lbfgs\" , max_iter = 500 , tol = 1e-6)\n",
    "logreg.fit(X_train, Y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_fit = logreg.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "accuracy_score(y_fit, Y_train)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(penalty = \"l2\" , C = .1,solver = \"lbfgs\" , max_iter = 500 , tol = 1e-6)\n",
    "logreg.fit(X_train, Y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_fit = logreg.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#accuracy_score(y_fit, Y_train)\n",
    "#le meilleur score trouvé pour cette méthode (trouvé de manière exploratoire), sans utiliser de VC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faisons une ACP pour avoir une intuition sur la séparabilité linéaire des données\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "pca.fit(X_train)\n",
    "X = pca.transform(X_train)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0] , X[:,1] ,c=Y_train,cmap='rainbow')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#3D\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter3D(X[:,0], X[:,1], X[:,2], c=Y_train , cmap = \"rainbow\")\n",
    "## ce ne sont que des outils de visualisation et au vue de notre score sur les Dapp, on pourrait penser que les donnéees\n",
    "#ne sont pas linéairement séparables (on utilise un classifieur linéaire) -- > information très importante\n",
    "## il y a de fortes chances que l'algorithme itératif de la régression logistique n'ait atteint qu'un minimum\n",
    "#local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6911764705882353"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def svc_param_selection_lin(X, y,Cs):\n",
    "    Cs = list(np.unique([round(x,3) for x in Cs]))\n",
    "    parameters = {'kernel':['linear'], 'C':Cs}\n",
    "    svc = svm.SVC(gamma = \"auto\")\n",
    "    clf = GridSearchCV(svc, parameters , return_train_score = True , cv = KFold(n_splits = 300))\n",
    "    clf.fit(X_train, Y_train)\n",
    "    return clf.best_params_\n",
    "\n",
    "C = svc_param_selection_lin(X_train, Y_train , np.linspace(0.01,11,4))[\"C\"]\n",
    "#Naturellement, on se dirigera vers les SVM non linéaires de manière à déterminer un hyperplan dans un espace \n",
    "#de plus grande dimension\n",
    "\n",
    "clf = LinearSVC(C = C , max_iter = 10000)\n",
    "clf.fit(X_train , Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_fit = clf.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "\n",
    "\n",
    "\n",
    "clf = LinearSVC(C = 0.01 , max_iter = 1000)\n",
    "clf.fit(X_train , Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_fit = clf.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#même performance que la régression logistique, avec un paramètre C trouvé de manière exploratoire\n",
    "\n",
    "\n",
    "\n",
    "#accuracy_score(y_fit, Y_train)\n",
    "#on constate donc que les performances sont similaires, on aura beau augmenté la constante de tolérance cela\n",
    "#ne fera pas augmenter le taux de bon classés dans les données d'apprentissage au contraire ...\n",
    "#on insiste bien sur l'erreur d'ajustement car c'est un moyen de juger la non-séparabilité \n",
    "#avec les données de départ, il semble difficile de déterminer un hyperplan comme en régression logistique\n",
    "#on décide donc d'opter pour les SVM à noyaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411765"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def svc_param_selection(X, y,Cs):\n",
    "    Cs = list(np.unique([round(x,3) for x in Cs]))\n",
    "    parameters = {'kernel':['rbf'], 'C':Cs}\n",
    "    svc = svm.SVC(gamma = \"auto\")\n",
    "    clf = GridSearchCV(svc, parameters , return_train_score = True , cv = KFold(n_splits = 300))\n",
    "    clf.fit(X_train, Y_train)\n",
    "    return clf.best_params_\n",
    "C = svc_param_selection(X_train, Y_train , np.linspace(5,11,4))[\"C\"]\n",
    "\n",
    "ksvm = svm.SVC(kernel=\"rbf\", gamma = \"auto\" , C = C)               \n",
    "ksvm.fit(X_train, Y_train)\n",
    "y_pred = ksvm.predict(X_test)\n",
    "y_fit = ksvm.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#accuracy_score(y_fit, Y_train)\n",
    "#le choix de gamma peut s'avérer très compliqué, il en est de même pour le choix du noyau ainsi que de la tolérance,\n",
    "#on se limite donc à chercher une constante de tolérance optimal pour un noyau RBF\n",
    "#on pourrait vérifier que si C est grand alors le taux de classification est égal à 1\n",
    "#on voit aussi que les performances sur les données de test sont légèrement moins bonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benco\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7352941176470589"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def svc_param_selection_poly(X, y,Cs , degree):\n",
    "    Cs = list(np.unique([round(x,3) for x in Cs]))\n",
    "    parameters = {'kernel':['poly'], 'C':Cs , \"degree\" : degree}\n",
    "    svc = svm.SVC(gamma = \"auto\")\n",
    "    clf = GridSearchCV(svc, parameters , return_train_score = True , cv = KFold(n_splits = 300))\n",
    "    clf.fit(X_train, Y_train)\n",
    "    return clf.best_params_\n",
    "params_poly = svc_param_selection_poly(X_train, Y_train , np.linspace(5,8,4) , list(range(1,3)))\n",
    "ksvm_poly = svm.SVC(kernel=\"poly\", degree = params_poly[\"degree\"] , C = params_poly[\"C\"])               \n",
    "ksvm_poly.fit(X_train, Y_train)\n",
    "y_pred = ksvm_poly.predict(X_test)\n",
    "y_fit = ksvm_poly.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#accuracy_score(y_fit, Y_train)\n",
    "## sur la base de nos grilles choisies, le meilleur score obtenu provient d'une SVM linéaire classique\n",
    "## on décide de ne pas optimiser la constante du polynôme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_m(liste): #renvoie le m minimisant l'erreur OOB\n",
    "    l = []\n",
    "    for m in liste:\n",
    "        clf = RandomForestClassifier(random_state = 0,\n",
    "                             n_estimators = 100 , max_features = m , min_samples_split = 2,\n",
    "                            oob_score = True)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        l.append((m , clf.oob_score_))\n",
    "    return min(l , key = lambda x : x[1])[0]\n",
    "\n",
    "m = oob_m(range(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411765"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state = 0,\n",
    "                             n_estimators = 100 , max_features = m , min_samples_split = 2,\n",
    "                            oob_score = True)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_fit = clf.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#accuracy_score(y_fit, Y_train)\n",
    "\n",
    "#dans notre cas de figure, déterminer m à l'aide de l'erreur OOB peut être une source de sur-apprentissage\n",
    "#on optera pour une validation croisée pour déterminer m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_CV(ms):\n",
    "    rfc = RandomForestClassifier(random_state = 0, min_samples_split = 2,\n",
    "                            oob_score = True , n_estimators = 100)\n",
    "    param_grid = {'max_features': ms}\n",
    "    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= KFold(n_splits = 3))\n",
    "    CV_rfc.fit(X_train, Y_train)\n",
    "    return CV_rfc.best_params_[\"max_features\"]\n",
    "#on décide de faire une 3-fold car la LOO est coûteuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = list(range(1,6))\n",
    "m = rfc_CV(ms)\n",
    "rfc = RandomForestClassifier(random_state = 0,\n",
    "                             n_estimators = 100 , max_features = m , min_samples_split = 2,\n",
    "                            oob_score = True)\n",
    "rfc.fit(X_train, Y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_fit = rfc.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "accuracy_score(y_fit, Y_train)\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(random_state = 0,\n",
    "                             n_estimators = 100 , max_features = 10 , min_samples_split = 2,\n",
    "                            oob_score = True)\n",
    "rfc.fit(X_train, Y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_fit = rfc.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "accuracy_score(y_fit, Y_train)\n",
    "\n",
    "#le meilleur modèle jusqu'à présent sur les données de test\n",
    "#on constatera que même avec m = 44, l'erreur d'ajustement est nulle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_CV(ks):\n",
    "    neigh = KNeighborsClassifier()\n",
    "    param_grid = {'n_neighbors': ks}\n",
    "    CV_knn = GridSearchCV(estimator=neigh, param_grid=param_grid, cv= KFold(n_splits = 300))\n",
    "    CV_knn.fit(X_train, Y_train)\n",
    "    return CV_knn.best_params_[\"n_neighbors\"]\n",
    "k = knn_CV(list(range(2,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6176470588235294"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "neigh.fit(X_train, Y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "y_fit = neigh.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#accuracy_score(y_fit , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205882352941176"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=100)\n",
    "neigh.fit(X_train, Y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "y_fit = neigh.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#accuracy_score(y_fit, Y_train)\n",
    "#on peut déterminer une valeur extrême k=100 qui nous permet d'avoir un score de .72 : mais\n",
    "#on est en sous-apprentissage en faisant ça"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_CV(ns):\n",
    "    ada = AdaBoostClassifier(random_state=0)\n",
    "    param_grid = {'n_estimators': ns}\n",
    "    CV_ada = GridSearchCV(estimator=ada, param_grid=param_grid, cv= KFold(n_splits = 5))\n",
    "    CV_ada.fit(X_train, Y_train)\n",
    "    return CV_ada.best_params_[\"n_estimators\"]\n",
    "B = ada_CV(list(range(50,70)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators = 69, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_fit = clf.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "\n",
    "#de manière exploratoire, on trouve 0.75 mais cette fois-ci, on craint moins le sur-apprentissage\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators = 66, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_fit = clf.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(accuracy_score(y_fit, Y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
