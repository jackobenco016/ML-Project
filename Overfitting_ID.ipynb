{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "from impyute.imputation.cs import mice#\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer#\n",
    "import missingno as msno#\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from missingpy import KNNImputer\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.set_option(\"display.max_rows\", 8)\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "from missingpy import MissForest\n",
    "import io\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn import decomposition\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dapp = pd.read_csv(\"horse-colic.data\" , sep = \"\\s+\", names = [\"V\"+str(i) for i in range(28)])\n",
    "Dtest = pd.read_csv(\"horse-colic.test\" , sep = \"\\s+\", names = [\"V\"+str(i) for i in range(28)])\n",
    "liste_drop = [\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\"]\n",
    "Dapp = Dapp.drop(liste_drop , axis = 1)\n",
    "Dtest = Dtest.drop(liste_drop , axis = 1)\n",
    "Dapp = Dapp.replace(\"?\",np.nan)\n",
    "Dtest = Dtest.replace(\"?\",np.nan)\n",
    "\n",
    "#Dtest\n",
    "#Dapp\n",
    "#On utilisera une méthode d'imputation qui gère différentes types pour les variables ! (cf Référence)\n",
    "#On se doit tout de même spécifier le type des variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V3', 'V4', 'V5', 'V15', 'V18', 'V19', 'V21']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "liste_categorical = [] ; liste_numerical = []\n",
    "for i in range(22):\n",
    "    if(i in list(range(3)) + list(range(6 , 15)) + [16 , 17 , 20]):\n",
    "        d[\"V\"+str(i)] = \"category\"\n",
    "        liste_categorical.append(\"V\"+str(i))\n",
    "    else:\n",
    "        d[\"V\"+str(i)] = \"float64\"\n",
    "        liste_numerical.append(\"V\"+str(i))\n",
    "liste_categorical\n",
    "liste_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_V2 = list(set(list(Dapp.V2.unique()) + list(Dtest.V2.unique()) ))\n",
    "label_V2 = [str(x) for x in label_V2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapp = Dapp.astype(d)\n",
    "Dtest = Dtest.astype(d)\n",
    "Dapp_num = Dapp.select_dtypes(include=['float64']) ; Dapp_cat = Dapp.select_dtypes(include=['category'])\n",
    "Dtest_num = Dtest.select_dtypes(include=['float64']) ; Dtest_cat = Dtest.select_dtypes(include=['category'])\n",
    "Dtest_cat.loc[: , \"V9\"] = pd.Series(pd.Categorical(Dtest_cat.loc[: , \"V9\"], categories=[\"1\",\"2\",\"3\"]))\n",
    "Dtest_cat.loc[: , \"V2\"] = Dtest_cat.loc[: , \"V2\"].astype(\"str\")\n",
    "Dapp_cat.loc[: , \"V2\"] = Dapp_cat.loc[: , \"V2\"].astype(\"str\")\n",
    "Dtest_cat.loc[: , \"V2\"] = pd.Series(pd.Categorical(Dtest_cat.loc[: , \"V2\"], categories=label_V2))\n",
    "Dapp_cat.loc[: , \"V2\"] = pd.Series(pd.Categorical(Dapp_cat.loc[: , \"V2\"], categories=label_V2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(l):\n",
    "    return np.array([float(x) for x in l])\n",
    "\n",
    "imputed_training=mice(np.array(list(map(g , Dapp_num.values))))\n",
    "imputed_test = mice(np.array(list(map(g , Dtest_num.values))))\n",
    "\n",
    "Dapp_num_imputed = pd.DataFrame(list(map(np.ravel, (list(imputed_training))))).round(2)\n",
    "Dapp_num_imputed.columns = liste_numerical\n",
    "\n",
    "Dtest_num_imputed = pd.DataFrame(list(map(np.ravel, (list(imputed_test))))).round(2)\n",
    "Dtest_num_imputed.columns = liste_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benco\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Benco\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Benco\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Benco\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## On a généré quelques valeurs négatives, il est nécessaire d'effectuer de nouvelles imputations sur nos données\n",
    "Dapp_num_imputed[Dapp_num_imputed < 0] = np.nan\n",
    "Dtest_num_imputed[Dtest_num_imputed < 0] = np.nan\n",
    "\n",
    "distances_app = pdist(Dapp_num_imputed.values, metric='euclidean')\n",
    "dist_matrix_app = squareform(distances_app)\n",
    "matrice_distance_app = pd.DataFrame(list(map(np.ravel, (list(dist_matrix_app))))).round(2)\n",
    "\n",
    "distances_test = pdist(Dtest_num_imputed.values, metric='euclidean')\n",
    "dist_matrix_test = squareform(distances_test)\n",
    "matrice_distance_test = pd.DataFrame(list(map(np.ravel, (list(dist_matrix_test))))).round(2)\n",
    "#on ne peut pas se permettre de prendre des poids uniformes\n",
    "#en effet, il se peut que le 2e plus proche soit en réalité très loin du point par rapport au premier\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=15, weights=\"distance\")\n",
    "Dapp_num_imputed = pd.DataFrame(list(map(np.ravel, (list(imputer.fit_transform(Dapp_num_imputed)))))).round(2)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3, weights=\"distance\")\n",
    "Dtest_num_imputed = pd.DataFrame(list(map(np.ravel, (list(imputer.fit_transform(Dtest_num_imputed)))))).round(2)\n",
    "\n",
    "Dapp_num_imputed.columns = liste_numerical ; Dtest_num_imputed.columns = liste_numerical\n",
    "#le choix du nombre de voisins est difficile, on fait donc des choix arbitraires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{lab : 1-(len(Dtest_num[lab].dropna())/len(Dtest_num)) for lab in liste_numerical} \n",
    "{lab : 1-(len(Dapp_num[lab].dropna())/len(Dapp_num)) for lab in liste_numerical} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dapp_num.V21\n",
    "y = Dapp_num_imputed.V21\n",
    "#L'objectif est de comparer les distributions pour voir si l'on observe une certaine cohérence après imputation\n",
    "#on compare les distributions avant/après\n",
    "#on calcule aussi la proportion de données manquantes pour être sûr que la comparaison ait un sens.\n",
    "#il faut que le % de données manquantes ne soit ni trop faible ni trop élevé pour pouvoir comparer\n",
    "#on peut faire une \"évaluation\" visuelle de notre imputation sur cette variable choisie\n",
    "plt.hist([x, y], label=['Missing data', 'Imputed data'] , color = [\"plum\",\"peru\"])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n"
     ]
    }
   ],
   "source": [
    "#Dapp_cat = Dapp_cat.drop([\"V2\"] , axis = 1) ; Dtest_cat = Dtest_cat.drop([\"V2\"] , axis = 1)\n",
    "cat_cols = [Dapp_cat.columns.get_loc(col) for col in Dapp_cat.select_dtypes(['category']).columns.tolist()]\n",
    "\n",
    "imputer = MissForest(random_state = 100)\n",
    "Dapp_cat_imputed = imputer.fit_transform(Dapp_cat , cat_vars = cat_cols)\n",
    "Dapp_cat_imputed = pd.DataFrame(list(map(np.ravel, (list(Dapp_cat_imputed)))))\n",
    "Dapp_cat_imputed.columns = [x for x in liste_categorical] #if(x!=\"V2\")\n",
    "Dapp_cat_imputed = Dapp_cat_imputed.apply(lambda x : x.astype(int).astype(str).astype(\"category\") , axis = 1)\n",
    "Dapp_cat_imputed.loc[: , \"V2\"] = Dapp_cat_imputed.loc[: , \"V2\"].astype(\"str\")\n",
    "Dapp_cat_imputed.loc[: , \"V2\"] = pd.Series(pd.Categorical(Dapp_cat_imputed.loc[: , \"V2\"], categories=label_V2))\n",
    "\n",
    "\n",
    "Dtest_cat_imputed = imputer.transform(Dtest_cat)\n",
    "Dtest_cat_imputed = pd.DataFrame(list(map(np.ravel, (list(Dtest_cat_imputed)))))\n",
    "Dtest_cat_imputed.columns = [x for x in liste_categorical] #if(x!=\"V2\")\n",
    "Dtest_cat_imputed = Dtest_cat_imputed.apply(lambda x : x.astype(int).astype(str).astype(\"category\") , axis = 1)\n",
    "Dtest_cat_imputed.loc[: , \"V9\"] = pd.Series(pd.Categorical(Dtest_cat_imputed.loc[: , \"V9\"], categories=[\"1\",\"2\",\"3\"]))\n",
    "Dtest_cat_imputed.loc[: , \"V2\"] = Dtest_cat_imputed.loc[: , \"V2\"].astype(\"str\")\n",
    "Dtest_cat_imputed.loc[: , \"V2\"] = pd.Series(pd.Categorical(Dtest_cat_imputed.loc[: , \"V2\"], categories=label_V2))\n",
    "#une modalité de V9 n'est pas présente dans la variable V9 des données de test : il faut rajouter une catégorie\n",
    "#pour la variable V9 des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{lab : 1-(len(Dtest_cat[lab].dropna())/len(Dtest)) for lab in liste_categorical if(lab != \"V2\")} \n",
    "{lab : 1-(len(Dapp_cat[lab].dropna())/len(Dapp)) for lab in liste_categorical if(lab != \"V2\")}\n",
    "#les proportions de données manquantes pour chaque varible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2369c871e48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 247,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dapp_cat_imputed.V13.value_counts().sort_index().plot(kind = \"bar\")\n",
    "#en pratique, il est nécessaire de vérifier la distribution des variables avant\\après imputation\n",
    "#on trace donc des barplot pour avoir une idée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2369c5a3288>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 247,
       "width": 376
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dapp_cat[\"V13\"].value_counts().sort_index().plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>528626</td>\n",
       "      <td>38.5</td>\n",
       "      <td>54.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.90</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>527950</td>\n",
       "      <td>37.6</td>\n",
       "      <td>48.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>535263</td>\n",
       "      <td>37.7</td>\n",
       "      <td>44.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.91</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>534523</td>\n",
       "      <td>37.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.31</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>528452</td>\n",
       "      <td>37.8</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.47</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>534783</td>\n",
       "      <td>38.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.37</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>44.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>528926</td>\n",
       "      <td>38.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.39</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>530670</td>\n",
       "      <td>37.6</td>\n",
       "      <td>88.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   V0 V1      V2    V3    V4    V5 V6 V7 V8 V9  ... V12 V13 V14   V15 V16  \\\n",
       "0   2  1  528626  38.5  54.0  20.0  3  1  2  2  ...   1   2   2  5.90   4   \n",
       "1   2  1  527950  37.6  48.0  36.0  1  1  1  1  ...   2   2   1  5.73   3   \n",
       "2   1  1  535263  37.7  44.0  28.0  3  4  3  2  ...   4   1   1  4.91   3   \n",
       "3   1  1  534523  37.0  56.0  24.0  3  1  4  2  ...   3   1   1  4.31   4   \n",
       ".. .. ..     ...   ...   ...   ... .. .. .. ..  ...  ..  ..  ..   ...  ..   \n",
       "64  2  1  528452  37.8  42.0  40.0  1  1  1  1  ...   1   2   1  7.47   3   \n",
       "65  1  1  534783  38.0  60.0  12.0  1  1  2  1  ...   1   1   1  4.37   1   \n",
       "66  2  1  528926  38.0  42.0  12.0  3  1  3  1  ...   1   2   1  6.39   4   \n",
       "67  2  1  530670  37.6  88.0  36.0  3  1  1  1  ...   2   1   3  1.50   4   \n",
       "\n",
       "    V17   V18   V19  V20    V21  \n",
       "0     2  42.0   6.3    2   4.61  \n",
       "1     5  44.0   6.3    1   5.00  \n",
       "2     5  45.0  70.0    3   2.00  \n",
       "3     5  35.0  61.0    3   2.00  \n",
       "..  ...   ...   ...  ...    ...  \n",
       "64    3  36.0   6.2    1   2.97  \n",
       "65    4  44.0  65.0    3   2.00  \n",
       "66    1  37.0   5.8    2   3.20  \n",
       "67    4  44.0   6.0    2  10.20  \n",
       "\n",
       "[68 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dapp_imputed = pd.concat([Dapp_num_imputed.reset_index(drop=True), Dapp_cat_imputed], axis=1)\n",
    "Dapp_imputed.columns = [int(x[1:]) for x in Dapp_imputed.columns]\n",
    "Dapp_imputed = Dapp_imputed.sort_index(axis = 1)\n",
    "Dapp_imputed.columns = [\"V\"+str(x) for x in Dapp_imputed.columns]\n",
    "\n",
    "Dtest_imputed = pd.concat([Dtest_num_imputed.reset_index(drop=True), Dtest_cat_imputed], axis=1)\n",
    "Dtest_imputed.columns = [int(x[1:]) for x in Dtest_imputed.columns]\n",
    "Dtest_imputed = Dtest_imputed.sort_index(axis = 1)\n",
    "Dtest_imputed.columns = [\"V\"+str(x) for x in Dtest_imputed.columns]\n",
    "Dtest_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dapp_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dtest_imputed.to_pickle(\"Dtest_imputed.pkl\")\n",
    "#Dapp_imputed.to_pickle(\"Dapp_imputed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dapp_imputed = pd.read_pickle(\"Dapp_imputed.pkl\")\n",
    "Dtest_imputed = pd.read_pickle(\"Dtest_imputed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_cat_bis = [x for x in liste_categorical if(x not in [\"V0\"] )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V15</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V21</th>\n",
       "      <th>V1_9</th>\n",
       "      <th>V2_530439</th>\n",
       "      <th>...</th>\n",
       "      <th>V14_3</th>\n",
       "      <th>V16_2</th>\n",
       "      <th>V16_3</th>\n",
       "      <th>V16_4</th>\n",
       "      <th>V17_2</th>\n",
       "      <th>V17_3</th>\n",
       "      <th>V17_4</th>\n",
       "      <th>V17_5</th>\n",
       "      <th>V20_2</th>\n",
       "      <th>V20_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.499926</td>\n",
       "      <td>-0.646563</td>\n",
       "      <td>-0.653774</td>\n",
       "      <td>0.685917</td>\n",
       "      <td>-0.431954</td>\n",
       "      <td>-0.697744</td>\n",
       "      <td>1.137392</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.869317</td>\n",
       "      <td>-0.861507</td>\n",
       "      <td>0.336586</td>\n",
       "      <td>0.596896</td>\n",
       "      <td>-0.230903</td>\n",
       "      <td>-0.697744</td>\n",
       "      <td>1.410798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.717179</td>\n",
       "      <td>-1.004802</td>\n",
       "      <td>-0.158594</td>\n",
       "      <td>0.167499</td>\n",
       "      <td>-0.130378</td>\n",
       "      <td>1.762025</td>\n",
       "      <td>-0.692325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.782146</td>\n",
       "      <td>-0.574915</td>\n",
       "      <td>-0.406184</td>\n",
       "      <td>-0.146693</td>\n",
       "      <td>-1.135631</td>\n",
       "      <td>1.414491</td>\n",
       "      <td>-0.692325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.565041</td>\n",
       "      <td>-1.076450</td>\n",
       "      <td>0.584176</td>\n",
       "      <td>1.508054</td>\n",
       "      <td>-1.035106</td>\n",
       "      <td>-0.701606</td>\n",
       "      <td>-0.012315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.260765</td>\n",
       "      <td>-0.431619</td>\n",
       "      <td>-1.148953</td>\n",
       "      <td>-0.115274</td>\n",
       "      <td>-0.230903</td>\n",
       "      <td>1.568950</td>\n",
       "      <td>-0.692325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.260765</td>\n",
       "      <td>-1.076450</td>\n",
       "      <td>-1.148953</td>\n",
       "      <td>0.942508</td>\n",
       "      <td>-0.934581</td>\n",
       "      <td>-0.717052</td>\n",
       "      <td>0.148924</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.869317</td>\n",
       "      <td>0.571452</td>\n",
       "      <td>0.336586</td>\n",
       "      <td>-1.618161</td>\n",
       "      <td>-0.230903</td>\n",
       "      <td>-0.709329</td>\n",
       "      <td>5.056210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 390 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V0        V3        V4        V5       V15       V18       V19       V21  \\\n",
       "0    0  0.499926 -0.646563 -0.653774  0.685917 -0.431954 -0.697744  1.137392   \n",
       "1    0 -0.869317 -0.861507  0.336586  0.596896 -0.230903 -0.697744  1.410798   \n",
       "2    1 -0.717179 -1.004802 -0.158594  0.167499 -0.130378  1.762025 -0.692325   \n",
       "3    1 -1.782146 -0.574915 -0.406184 -0.146693 -1.135631  1.414491 -0.692325   \n",
       "..  ..       ...       ...       ...       ...       ...       ...       ...   \n",
       "64   0 -0.565041 -1.076450  0.584176  1.508054 -1.035106 -0.701606 -0.012315   \n",
       "65   1 -0.260765 -0.431619 -1.148953 -0.115274 -0.230903  1.568950 -0.692325   \n",
       "66   0 -0.260765 -1.076450 -1.148953  0.942508 -0.934581 -0.717052  0.148924   \n",
       "67   0 -0.869317  0.571452  0.336586 -1.618161 -0.230903 -0.709329  5.056210   \n",
       "\n",
       "    V1_9  V2_530439  ...  V14_3  V16_2  V16_3  V16_4  V17_2  V17_3  V17_4  \\\n",
       "0      0          0  ...      0      0      0      1      1      0      0   \n",
       "1      0          0  ...      0      0      1      0      0      0      0   \n",
       "2      0          0  ...      0      0      1      0      0      0      0   \n",
       "3      0          0  ...      0      0      0      1      0      0      0   \n",
       "..   ...        ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "64     0          0  ...      0      0      1      0      0      1      0   \n",
       "65     0          0  ...      0      0      0      0      0      0      1   \n",
       "66     0          0  ...      0      0      0      1      0      0      0   \n",
       "67     0          0  ...      1      0      0      1      0      0      1   \n",
       "\n",
       "    V17_5  V20_2  V20_3  \n",
       "0       0      1      0  \n",
       "1       1      0      0  \n",
       "2       1      0      1  \n",
       "3       1      0      1  \n",
       "..    ...    ...    ...  \n",
       "64      0      0      0  \n",
       "65      0      0      1  \n",
       "66      0      1      0  \n",
       "67      0      1      0  \n",
       "\n",
       "[68 rows x 390 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "Dapp_imputed[liste_numerical] = scaler.fit_transform(Dapp_imputed[liste_numerical])\n",
    "Dapp_imputed[\"V0\"] = pd.Series([1 if(x==\"1\") else 0 for x in Dapp_imputed[\"V0\"]])\n",
    "Dapp_imputed = Dapp_imputed.astype({\"V0\" : \"int32\"})\n",
    "Dapp_imputed = pd.get_dummies(Dapp_imputed , columns = liste_cat_bis , drop_first = True)\n",
    "\n",
    "Dtest_imputed[liste_numerical] = scaler.transform(Dtest_imputed[liste_numerical])\n",
    "Dtest_imputed[\"V0\"] = pd.Series([1 if(x==\"1\") else 0 for x in Dtest_imputed[\"V0\"]])\n",
    "Dtest_imputed = Dtest_imputed.astype({\"V0\" : \"int32\"})\n",
    "Dtest_imputed = pd.get_dummies(Dtest_imputed , columns = liste_cat_bis , drop_first = True)\n",
    "\n",
    "Dapp_imputed\n",
    "Dtest_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V15</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V21</th>\n",
       "      <th>V1_9</th>\n",
       "      <th>V2_530439</th>\n",
       "      <th>...</th>\n",
       "      <th>V14_3</th>\n",
       "      <th>V16_2</th>\n",
       "      <th>V16_3</th>\n",
       "      <th>V16_4</th>\n",
       "      <th>V17_2</th>\n",
       "      <th>V17_3</th>\n",
       "      <th>V17_4</th>\n",
       "      <th>V17_5</th>\n",
       "      <th>V20_2</th>\n",
       "      <th>V20_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.499926</td>\n",
       "      <td>-0.216675</td>\n",
       "      <td>-0.158594</td>\n",
       "      <td>0.570713</td>\n",
       "      <td>-0.130378</td>\n",
       "      <td>-0.616653</td>\n",
       "      <td>0.352226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.564893</td>\n",
       "      <td>0.571452</td>\n",
       "      <td>-0.653774</td>\n",
       "      <td>-1.586742</td>\n",
       "      <td>0.372249</td>\n",
       "      <td>2.341248</td>\n",
       "      <td>-0.692325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.195650</td>\n",
       "      <td>-1.148098</td>\n",
       "      <td>-0.406184</td>\n",
       "      <td>0.743519</td>\n",
       "      <td>-1.336682</td>\n",
       "      <td>-0.682298</td>\n",
       "      <td>0.176966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.412754</td>\n",
       "      <td>3.294073</td>\n",
       "      <td>3.307665</td>\n",
       "      <td>0.214628</td>\n",
       "      <td>0.171198</td>\n",
       "      <td>-0.662991</td>\n",
       "      <td>1.621110</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.477870</td>\n",
       "      <td>-0.001731</td>\n",
       "      <td>-0.406184</td>\n",
       "      <td>-0.240951</td>\n",
       "      <td>-0.230903</td>\n",
       "      <td>-0.072955</td>\n",
       "      <td>0.219029</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.021455</td>\n",
       "      <td>-0.001731</td>\n",
       "      <td>-0.034799</td>\n",
       "      <td>0.596896</td>\n",
       "      <td>1.377502</td>\n",
       "      <td>-0.678437</td>\n",
       "      <td>0.499445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.542836</td>\n",
       "      <td>1.001339</td>\n",
       "      <td>-0.406184</td>\n",
       "      <td>-0.686057</td>\n",
       "      <td>0.372249</td>\n",
       "      <td>-0.709329</td>\n",
       "      <td>0.289133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.477870</td>\n",
       "      <td>-1.148098</td>\n",
       "      <td>-0.653774</td>\n",
       "      <td>-1.372044</td>\n",
       "      <td>-1.035106</td>\n",
       "      <td>1.453105</td>\n",
       "      <td>-1.393365</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 390 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     V0        V3        V4        V5       V15       V18       V19       V21  \\\n",
       "0     0  0.499926 -0.216675 -0.158594  0.570713 -0.130378 -0.616653  0.352226   \n",
       "1     1  1.564893  0.571452 -0.653774 -1.586742  0.372249  2.341248 -0.692325   \n",
       "2     0  0.195650 -1.148098 -0.406184  0.743519 -1.336682 -0.682298  0.176966   \n",
       "3     1  1.412754  3.294073  3.307665  0.214628  0.171198 -0.662991  1.621110   \n",
       "..   ..       ...       ...       ...       ...       ...       ...       ...   \n",
       "296   0 -1.477870 -0.001731 -0.406184 -0.240951 -0.230903 -0.072955  0.219029   \n",
       "297   1 -1.021455 -0.001731 -0.034799  0.596896  1.377502 -0.678437  0.499445   \n",
       "298   1 -2.542836  1.001339 -0.406184 -0.686057  0.372249 -0.709329  0.289133   \n",
       "299   1 -1.477870 -1.148098 -0.653774 -1.372044 -1.035106  1.453105 -1.393365   \n",
       "\n",
       "     V1_9  V2_530439  ...  V14_3  V16_2  V16_3  V16_4  V17_2  V17_3  V17_4  \\\n",
       "0       0          0  ...      0      0      1      0      0      0      0   \n",
       "1       0          0  ...      0      0      0      1      1      0      0   \n",
       "2       0          0  ...      0      0      0      0      0      0      0   \n",
       "3       1          0  ...      0      0      1      0      0      0      0   \n",
       "..    ...        ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "296     0          0  ...      0      0      0      1      0      0      1   \n",
       "297     0          0  ...      0      0      1      0      0      0      0   \n",
       "298     0          0  ...      0      0      0      1      0      0      1   \n",
       "299     0          0  ...      0      0      0      1      0      0      0   \n",
       "\n",
       "     V17_5  V20_2  V20_3  \n",
       "0        1      0      1  \n",
       "1        0      1      0  \n",
       "2        0      1      0  \n",
       "3        1      0      1  \n",
       "..     ...    ...    ...  \n",
       "296      0      0      1  \n",
       "297      1      0      1  \n",
       "298      0      0      1  \n",
       "299      0      0      0  \n",
       "\n",
       "[300 rows x 390 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dapp_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = Dapp_imputed.drop([\"V0\"] , axis = 1) ; Y_train = Dapp_imputed[\"V0\"]\n",
    "X_test = Dtest_imputed.drop([\"V0\"] , axis = 1) ; Y_test = Dtest_imputed[\"V0\"]\n",
    "#Voici nos jeux de données d'entrainement ainsi que de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(np.dot(np.transpose(X_train) , X_train)) #la matrice de design n'est pas de rang plein, rien ne nous garantit que\n",
    "#que l'on atteindra un minimum global en faisant une régression logistique\n",
    "#on décide de pénaliser celle-ci avec une pénalité de type ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7941176470588235"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def param_selection_log(X, y,Cs):\n",
    "    logreg = LogisticRegression(penalty = \"l2\", solver = \"lbfgs\" , max_iter = 5000 , tol = 1e-6)\n",
    "    parameters = {'C':Cs}\n",
    "    clf = GridSearchCV(logreg, parameters, cv = KFold(n_splits = 300))\n",
    "    clf.fit(X_train, Y_train)\n",
    "    return clf.best_params_\n",
    "\n",
    "C_log = param_selection_log(X_train, Y_train , [.001 , 0.01 , 0.1 , 1 , 10 , 100])[\"C\"]\n",
    "#Naturellement, on se dirigera vers les SVM non linéaires de manière à déterminer un hyperplan dans un espace \n",
    "#de plus grande dimension\n",
    "\n",
    "logreg = LogisticRegression(penalty = \"l2\", solver = \"lbfgs\" , max_iter = 5000 , tol = 1e-6 , C = C_log)\n",
    "logreg.fit(X_train , Y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_fit = logreg.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_fit, Y_train) #on dipose d'une erreur d'apprentissage nulle\n",
    "#cela laisse présager du sur-apprentissage --> dès lors on peut remettre en doute notre performance\n",
    "#de plus, on constate qu'il existe une séparation linéaire des données\n",
    "#nous allons donc utiliser un autre classifieur linéaire : SVML\n",
    "#une SVM à noyaux est inutile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.Series([-1 if(y==0) else 1 for y in Y_train])\n",
    "Y_test = pd.Series([-1 if(y==0) else 1 for y in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8088235294117647"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def svc_param_selection_lin(X, y,Cs):\n",
    "    parameters = {'kernel':['linear'], 'C':Cs}\n",
    "    svc = svm.SVC(gamma = 1 , coef0 = 0)\n",
    "    clf = GridSearchCV(svc, parameters , return_train_score = True , cv = KFold(n_splits = 3))\n",
    "    clf.fit(X_train, Y_train)\n",
    "    return clf.best_params_\n",
    "##on effectue une VC 3-fold car moins coûteuse\n",
    "C_lin = svc_param_selection_lin(X_train, Y_train , [.001 , 0.01 , 0.1 , 1 , 10 , 100])[\"C\"]\n",
    "\n",
    "svm_lin = LinearSVC(C = C_lin , max_iter = 10000)\n",
    "svm_lin.fit(X_train , Y_train)\n",
    "y_pred = svm_lin.predict(X_test)\n",
    "y_fit = svm_lin.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#même performance que la régression logistique et même problème que la régression logistique\n",
    "#tout laisse penser que l'on est face à un problème de sur-apprentissage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_fit, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7941176470588235"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def svc_param_selection_poly(X, y,Cs , degree):\n",
    "    Cs = list(np.unique([round(x,3) for x in Cs]))\n",
    "    parameters = {'kernel':['poly'], 'C':Cs , \"degree\" : degree}\n",
    "    svc = svm.SVC(gamma = \"auto\")\n",
    "    clf = GridSearchCV(svc, parameters , return_train_score = True , cv = KFold(n_splits = 3))\n",
    "    clf.fit(X_train, Y_train)\n",
    "    return clf.best_params_\n",
    "params_poly = svc_param_selection_poly(X_train, Y_train , [.001 , 0.01 , 0.1 , 1 , 10 , 100 , 1000] , list(range(1,4)))\n",
    "ksvm_poly = svm.SVC(kernel=\"poly\", degree = params_poly[\"degree\"] , C = params_poly[\"C\"] , gamma = \"auto\")               \n",
    "ksvm_poly.fit(X_train, Y_train)\n",
    "y_pred = ksvm_poly.predict(X_test)\n",
    "y_fit = ksvm_poly.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "## même constat pour ce modèle, on retrouve une SVM linéaire avec une erreur d'ajustement nulle\n",
    "#choisir le paramètre du modèle selon la validation croisée nous amnère dans les 3 denrières méthodes utilisées à choisir\n",
    "#la complexité la plus grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_fit, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_m(liste): #renvoie le m minimisant l'erreur OOB\n",
    "    l = []\n",
    "    for m in liste:\n",
    "        clf = RandomForestClassifier(random_state = 0,\n",
    "                             n_estimators = 100 , max_features = m , min_samples_split = 2,\n",
    "                            oob_score = True)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        l.append((m , clf.oob_score_))\n",
    "    return min(l , key = lambda x : x[1])[0]\n",
    "\n",
    "m = oob_m(range(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7352941176470589"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state = 0,\n",
    "                             n_estimators = 100 , max_features = m , min_samples_split = 2,\n",
    "                            oob_score = True)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_fit = clf.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#accuracy_score(y_fit, Y_train)\n",
    "\n",
    "#moins performante que les précédentes mais l'erreur d'ajustement est nulle aussi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_fit, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_CV(ms):\n",
    "    rfc = RandomForestClassifier(random_state = 0, min_samples_split = 2,\n",
    "                            oob_score = True , n_estimators = 100)\n",
    "    param_grid = {'max_features': ms}\n",
    "    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= KFold(n_splits = 3))\n",
    "    CV_rfc.fit(X_train, Y_train)\n",
    "    return CV_rfc.best_params_[\"max_features\"]\n",
    "#on décide de faire une 3-fold car la LOO est coûteuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647058823529411"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = list(range(1,6))\n",
    "m = rfc_CV(ms)\n",
    "rfc = RandomForestClassifier(random_state = 0,\n",
    "                             n_estimators = 100 , max_features = m , min_samples_split = 2,\n",
    "                            oob_score = True)\n",
    "rfc.fit(X_train, Y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_fit = rfc.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "accuracy_score(y_fit, Y_train)\n",
    "#idem que précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_CV(ks):\n",
    "    neigh = KNeighborsClassifier()\n",
    "    param_grid = {'n_neighbors': ks}\n",
    "    CV_knn = GridSearchCV(estimator=neigh, param_grid=param_grid, cv= KFold(n_splits = 3))\n",
    "    CV_knn.fit(X_train, Y_train)\n",
    "    return CV_knn.best_params_[\"n_neighbors\"]\n",
    "k = knn_CV(list(range(1,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7794117647058824"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "neigh.fit(X_train, Y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "y_fit = neigh.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n",
    "#accuracy_score(y_fit , Y_train)\n",
    "#aussi surprenant que cela puisse paraître le modèle qui ne semble pas souffrir de sur-apprentissage\n",
    "#est k-ppv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7433333333333333"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_fit , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_CV(ns):\n",
    "    ada = AdaBoostClassifier(random_state=0)\n",
    "    param_grid = {'n_estimators': ns}\n",
    "    CV_ada = GridSearchCV(estimator=ada, param_grid=param_grid, cv= KFold(n_splits = 3))\n",
    "    CV_ada.fit(X_train, Y_train)\n",
    "    return CV_ada.best_params_[\"n_estimators\"]\n",
    "B = ada_CV(list(range(1,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647058823529411"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators = B, random_state=0)\n",
    "ada.fit(X_train, Y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "y_fit = ada.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9633333333333334"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(accuracy_score(y_fit, Y_train))\n",
    "#ce modèle semble aussi souffrir de sur-apprentissage : avec une grille plus large, nous n'aurions pas d'erreur\n",
    "#d'ajustement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtree_grid_search(X,y,depth):\n",
    "    param_grid = {'max_depth': depth}\n",
    "    dtree_model=DecisionTreeClassifier(random_state = 0)\n",
    "    dtree_gscv = GridSearchCV(dtree_model, param_grid, cv= KFold(n_splits = 3))\n",
    "    dtree_gscv.fit(X, y)\n",
    "    return dtree_gscv.best_params_\n",
    "\n",
    "P = dtree_grid_search(X_train,Y_train,list(range(3,100)))[\"max_depth\"]\n",
    "\n",
    "arb = DecisionTreeClassifier(random_state=0 , max_depth = P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647058823529411"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arb.fit(X_train, Y_train)\n",
    "y_pred = arb.predict(X_test)\n",
    "y_fit = arb.predict(X_train)\n",
    "accuracy_score(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9366666666666666"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_fit, Y_train) #idem pour ce modèle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
